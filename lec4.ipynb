{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIouX+o4whau4+HigMy9D8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanvika15/nlp-lec4/blob/main/lec4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFrGn6UsHI-8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Cleaning**"
      ],
      "metadata": {
        "id": "Oc6nzzCuH0oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"\"\"<gfg>\n",
        "#GFG Geeks Learning together\n",
        "url <https://www.geeksforgeeks.org/>,\n",
        "email <acs@sdf.dv>\n",
        "\"\"\"\n",
        "def clean_text(text):\n",
        "    # remove HTML TAG\n",
        "    html = re.compile('[<,#*?>]')\n",
        "    text = html.sub(r'',text)\n",
        "    # Remove urls:\n",
        "    url = re.compile('https?://\\S+|www\\.S+')\n",
        "    text = url.sub(r'',text)\n",
        "    # Remove email id:\n",
        "    email = re.compile('[A-Za-z0-2]+@[\\w]+.[\\w]+')\n",
        "    text = email.sub(r'',text)\n",
        "    return text\n",
        "print(clean_text(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8TGKr6rHUtW",
        "outputId": "2ea7bb71-2167-46be-aa35-2f24cb413e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gfg \n",
            "GFG Geeks Learning together \n",
            "url  \n",
            "email \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing**"
      ],
      "metadata": {
        "id": "iSRs9nNtIBMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81O6bbVQILAU",
        "outputId": "6a002c98-b5e6-4d44-e703-34500496f97f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "import string\n",
        "\n",
        "# sample text to be preprocessed\n",
        "text = 'GeeksforGeeks is a very famous edutech company in the IT industry.'\n",
        "\n",
        "# tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# perform stemming and lemmatization\n",
        "stemmer = SnowballStemmer('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "# remove digits and punctuation\n",
        "cleaned_tokens = [token for token in lemmatized_tokens\n",
        "                  if not token.isdigit() and not token in string.punctuation]\n",
        "\n",
        "# convert all tokens to lowercase\n",
        "lowercase_tokens = [token.lower() for token in cleaned_tokens]\n",
        "\n",
        "# perform part-of-speech (POS) tagging\n",
        "pos_tags = pos_tag(lowercase_tokens)\n",
        "\n",
        "# perform named entity recognition (NER)\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "\n",
        "# print the preprocessed text\n",
        "print(\"Original text:\", text)\n",
        "print(\"Preprocessed tokens:\", lowercase_tokens)\n",
        "print(\"POS tags:\", pos_tags)\n",
        "print(\"Named entities:\", named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHESjqgaHVaO",
        "outputId": "6830105c-7a43-4051-a1b8-1834b0b843dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: GeeksforGeeks is a very famous edutech company in the IT industry.\n",
            "Preprocessed tokens: ['geeksforgeeks', 'famous', 'edutech', 'company', 'industry']\n",
            "POS tags: [('geeksforgeeks', 'NNS'), ('famous', 'JJ'), ('edutech', 'JJ'), ('company', 'NN'), ('industry', 'NN')]\n",
            "Named entities: (S geeksforgeeks/NNS famous/JJ edutech/JJ company/NN industry/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "455SDGIsIGSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EwRwRa1tJJQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**"
      ],
      "metadata": {
        "id": "IqXzqoIEJJwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Bag of Word(Bow)\n"
      ],
      "metadata": {
        "id": "vgvwqLqcJRa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#nltk.download('punkt') # Download 'punkt' from nltk if it's not downloaded\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "Text = \"\"\"GeeksForGeeks.\n",
        "Geeks Learning Together.\n",
        "GeeksForGeeks is famous for DSA.\n",
        "Learning DSA\"\"\"\n",
        "# TOKENIZATION\n",
        "sentences = sent_tokenize(Text)\n",
        "sentences = [sent.lower().replace(\".\",\"\") for sent in sentences]\n",
        "print('Our Corpus:',sentences)\n",
        "#CountVectorizer : Convert a collection of text documents to a matrix of token counts.\n",
        "count_vect = CountVectorizer()\n",
        "# fit & transform will represent each sentences as BOW representation\n",
        "BOW = count_vect.fit_transform(sentences)\n",
        "# Get the vocabulary\n",
        "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
        "#see the BOW representation\n",
        "print(f\"BoW representation for {sentences[0]} {BOW[0].toarray()}\")\n",
        "print(f\"BoW representation for {sentences[1]} {BOW[1].toarray()}\")\n",
        "print(f\"BoW representation for {sentences[2]} {BOW[2].toarray()}\")\n",
        "# BOW representation for a new text\n",
        "BOW_ = count_vect.transform([\"learning dsa from geeksforgeeks\"])\n",
        "print(\"Bow representation for 'learning dsa from geeksforgeeks':\", BOW_.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko6ervr1JjOC",
        "outputId": "2dd153e7-1361-4b6d-d5d2-8b6e2584d84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our Corpus: ['geeksforgeeks', 'geeks learning together', 'geeksforgeeks is famous for dsa', 'learning dsa']\n",
            "Our vocabulary:  {'geeksforgeeks': 4, 'geeks': 3, 'learning': 6, 'together': 7, 'is': 5, 'famous': 1, 'for': 2, 'dsa': 0}\n",
            "BoW representation for geeksforgeeks [[0 0 0 0 1 0 0 0]]\n",
            "BoW representation for geeks learning together [[0 0 0 1 0 0 1 1]]\n",
            "BoW representation for geeksforgeeks is famous for dsa [[1 1 1 0 1 1 0 0]]\n",
            "Bow representation for 'learning dsa from geeksforgeeks': [[1 0 0 0 1 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk.download('punkt') # Download 'punkt'\n",
        "# from nltk if it's not downloaded\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "Text = \"\"\"GeeksForGeeks.\n",
        "         Geeks Learning Together.\n",
        "         GeeksForGeeks is famous for DSA.\n",
        "         Learning DSA\"\"\"\n",
        "\n",
        "# TOKENIZATION\n",
        "sentences = sent_tokenize(Text)\n",
        "sentences = [sent.lower().replace(\".\", \"\") for sent in sentences]\n",
        "print('Our Corpus:', sentences)\n",
        "\n",
        "# Ngram vectorization example with count\n",
        "# vectorizer and uni, bi, trigrams\n",
        "count_vect = CountVectorizer(ngram_range=(1, 3))\n",
        "\n",
        "# fit & transform will represent each sentences\n",
        "# as Bag of n-grams representation\n",
        "BOW_nGram = count_vect.fit_transform(sentences)\n",
        "\n",
        "# Get the vocabulary\n",
        "print(\"Our vocabulary:\\n\", count_vect.vocabulary_)\n",
        "\n",
        "# see the Bag of n-grams representation\n",
        "print('Ngram representation for \"{}\" is {}'\n",
        "      .format(sentences[0], BOW_nGram[0].toarray()))\n",
        "print('Ngram representation for \"{}\" is {}'\n",
        "      .format(sentences[1], BOW_nGram[1].toarray()))\n",
        "print('Ngram representation for \"{}\" is {}'.\n",
        "      format(sentences[2], BOW_nGram[2].toarray()))\n",
        "\n",
        "# Bag of n-grams representation for a new text\n",
        "BOW_nGram_ = count_vect.transform([\"learning dsa from geeksforgeeks together\"])\n",
        "print(\"Ngram representation for  'learning dsa from geeksforgeeks together' is\",\n",
        "     BOW_nGram_.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzbmBKPFJs4W",
        "outputId": "4538527e-2476-4b56-8e9b-2f75f61f963b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our Corpus: ['geeksforgeeks', 'geeks learning together', 'geeksforgeeks is famous for dsa', 'learning dsa']\n",
            "Our vocabulary:\n",
            " {'geeksforgeeks': 9, 'geeks': 6, 'learning': 15, 'together': 18, 'geeks learning': 7, 'learning together': 17, 'geeks learning together': 8, 'is': 12, 'famous': 1, 'for': 4, 'dsa': 0, 'geeksforgeeks is': 10, 'is famous': 13, 'famous for': 2, 'for dsa': 5, 'geeksforgeeks is famous': 11, 'is famous for': 14, 'famous for dsa': 3, 'learning dsa': 16}\n",
            "Ngram representation for \"geeksforgeeks\" is [[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]]\n",
            "Ngram representation for \"geeks learning together\" is [[0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1]]\n",
            "Ngram representation for \"geeksforgeeks is famous for dsa\" is [[1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0]]\n",
            "Ngram representation for  'learning dsa from geeksforgeeks together' is [[1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQDv1bxjJykr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}